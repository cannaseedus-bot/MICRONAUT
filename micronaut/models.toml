# Model Configuration for MICRONAUT MoE Inference System
# Authority: KUHUL_π
# Status: Sealed declaration (mutation forbidden)
# Pattern: Mixture-of-Experts — 9 Micronauts as fold-scoped experts around Phi-2

[metadata]
schema = "fel-toml://models/v1"
authority = "KUHUL_π"
mutation = "forbidden"
pattern = "mixture_of_experts"
description = "Fold-enclosed, deterministic, static model owned entirely by the Micronaut system"

# ============================================================================
# PRIMARY MODEL: Phi-2 GGUF (2-bit quantized)
# ============================================================================

[model.phi2]
name = "phi-2-gguf"
source = "TheBloke/phi-2-GGUF"
format = "gguf"
quantization = "Q2_K"
parameters = "2.7B"
context_length = 2048
vocab_size = 51200
architecture = "transformer"
dtype = "q2_k"
ownership = "static"
deterministic = true
fold_enclosed = true

# File paths (relative to project root)
model_path = "models/phi-2/phi-2.Q2_K.gguf"
tokenizer_path = "models/phi-2/tokenizer.json"
config_path = "models/phi-2/config.json"

# Inference constraints
max_tokens = 512
temperature = 0.0
top_p = 1.0
top_k = 1
repetition_penalty = 1.0
seed = 42

# Fold binding — MM-1 is the sole model interface
bound_micronaut = "MM-1"
bound_fold = "⟁COMPUTE_FOLD⟁"
bound_lane = "BATCH"

# ============================================================================
# RUNTIME: transformers.js (browser/Node.js compatible)
# ============================================================================

[runtime.transformers_js]
engine = "transformers.js"
version = ">=2.0.0"
backend = "wasm"
threads = 4
description = "GGUF loading via @huggingface/transformers WASM backend"

[runtime.transformers_js.gguf_support]
enabled = true
quantization_types = ["Q2_K", "Q4_K_M", "Q5_K_M", "Q8_0"]
memory_map = true

# ============================================================================
# INFERENCE CLUSTER: 1000-Node JSON Runtime Object Cluster
# ============================================================================

[cluster]
name = "kuhul-inference-cluster"
description = "JSON runtime objects as a distributed inference grid (hybrid CUDA-core pattern)"
total_nodes = 1000
topology = "3d_grid"
grid_dimensions = [10, 10, 10]

[cluster.node_types]
compute_nodes = 500
routing_nodes = 200
storage_nodes = 150
verification_nodes = 100
control_nodes = 50

[cluster.fold_allocation]
# Each fold gets a dedicated slice of the cluster
CONTROL_FOLD = { nodes = 50, type = "control", lane = "EDGE" }
DATA_FOLD = { nodes = 80, type = "routing", lane = "DICT" }
STORAGE_FOLD = { nodes = 150, type = "storage", lane = "FIELD" }
NETWORK_FOLD = { nodes = 40, type = "routing", lane = "EDGE" }
UI_FOLD = { nodes = 60, type = "routing", lane = "LANE" }
AUTH_FOLD = { nodes = 20, type = "routing", lane = "DICT" }
DB_FOLD = { nodes = 30, type = "storage", lane = "FIELD" }
COMPUTE_FOLD = { nodes = 300, type = "compute", lane = "BATCH" }
STATE_FOLD = { nodes = 50, type = "storage", lane = "FIELD" }
EVENTS_FOLD = { nodes = 30, type = "routing", lane = "LANE" }
TIME_FOLD = { nodes = 30, type = "routing", lane = "LANE" }
SPACE_FOLD = { nodes = 20, type = "routing", lane = "EDGE" }
META_FOLD = { nodes = 80, type = "verification", lane = "DICT" }
PATTERN_FOLD = { nodes = 40, type = "compute", lane = "DICT" }

[cluster.inference_pipeline]
description = "Token flow through the MoE expert pipeline"
stages = [
    "PM-1: perceive input → select fields → route to intent",
    "CM-1: gate input → resolve phase → permit/deny",
    "TM-1: schedule collapse timing → gate replay window",
    "HM-1: detect host → normalize IO for cluster",
    "MM-1: load Phi-2 → emit tokens → stream signals",
    "XM-1: expand output → generate metaphors (post-collapse)",
    "SM-1: seal result → snapshot → preserve byte identity",
    "VM-2: verify proof → attest hash → audit trace",
    "VM-1: render projection → emit frame (SVG/CSS/DOM/3D)"
]

# ============================================================================
# 3D VISUALIZATION: Three.js / kuhul-3D Inference Cluster
# ============================================================================

[visualization]
engine = "three.js"
renderer = "webgl"
description = "3D inference cluster rendering — nodes as cubes, folds as planes, tokens as particles"

[visualization.scene]
width = 1200
height = 800
background = "#0a0a0f"
camera_fov = 60
camera_distance = 25

[visualization.node_rendering]
compute_color = "#00ff88"
routing_color = "#4488ff"
storage_color = "#ff8844"
verification_color = "#ff44ff"
control_color = "#ffff00"
node_size = 0.15
active_glow = true
token_particle_size = 0.05
token_particle_speed = 2.0

[visualization.fold_planes]
enabled = true
opacity = 0.08
grid_visible = true

# ============================================================================
# MoE EXPERT ROUTING (Micronaut-as-Expert)
# ============================================================================

[moe]
description = "Mixture-of-Experts pattern: 9 Micronauts as fold-scoped experts"
gating_mechanism = "ngram_match_score"
gating_source = "micronaut/brains/meta-intent-map.json"
top_k_experts = 3
load_balancing = true

[[moe.experts]]
id = "CM-1"
role = "phase_geometry"
fold = "⟁CONTROL_FOLD⟁"
gate_priority = 1
expert_type = "control_gate"
description = "Pre-semantic gating — decides if inference is permitted"

[[moe.experts]]
id = "PM-1"
role = "field_selection"
fold = "⟁DATA_FOLD⟁"
gate_priority = 2
expert_type = "input_router"
description = "Perception gating — selects relevant input fields"

[[moe.experts]]
id = "TM-1"
role = "collapse_timing"
fold = "⟁TIME_FOLD⟁"
gate_priority = 3
expert_type = "scheduler"
description = "Temporal gating — schedules collapse and replay"

[[moe.experts]]
id = "HM-1"
role = "host_abstraction"
fold = "⟁STATE_FOLD⟁"
gate_priority = 4
expert_type = "environment"
description = "Host detection — normalizes IO for cluster topology"

[[moe.experts]]
id = "SM-1"
role = "inert_persistence"
fold = "⟁STORAGE_FOLD⟁"
gate_priority = 5
expert_type = "storage"
description = "Seals inference results — snapshot and byte identity"

[[moe.experts]]
id = "MM-1"
role = "token_signal_generator"
fold = "⟁COMPUTE_FOLD⟁"
gate_priority = 6
expert_type = "inference_core"
description = "Primary inference expert — Phi-2 GGUF token emission"

[[moe.experts]]
id = "XM-1"
role = "narrative_expansion"
fold = "⟁PATTERN_FOLD⟁"
gate_priority = 7
expert_type = "expansion"
description = "Post-collapse expansion — metaphors, analogies, narrative"

[[moe.experts]]
id = "VM-1"
role = "rendering_projection"
fold = "⟁UI_FOLD⟁"
gate_priority = 8
expert_type = "projection"
description = "Output rendering — SVG, CSS, DOM, terminal, 3D"

[[moe.experts]]
id = "VM-2"
role = "proof_generation"
fold = "⟁META_FOLD⟁"
gate_priority = 9
expert_type = "verification"
description = "Proof attestation — hash binding, replay determinism"

# ============================================================================
# S7-LLM-MOE-140M: Native sealed MoE model
# ============================================================================

[model.s7_llm_moe_140m]
name = "s7-llm-moe-140m"
class = "SCXQ7::S7_LLM"
variant = "MOE"
format = "s7l"
version = "1.0.0"
parameters_total = "~140M"
parameters_active_per_token = "~50M"
context_length = 2048
vocab_size = 24576
architecture = "mixture_of_experts"
quantization = "INT8"
training_dtype = "FP32"
deterministic = true
fold_enclosed = true
ownership = "static"

# File paths (relative to project root)
artifact_path = "s7-llm-moe/model/moe.s7l"
vocab_path    = "s7-llm-moe/model/vocab.json"
runtime_bin   = "s7-llm-moe/target/release/s7-llm-moe"
packer_bin    = "s7-llm-moe/target/release/s7-pack-moe"
training_spec = "docs/s7-llm-moe-training.md"

# Fold binding
bound_micronaut = "MM-1"
bound_fold      = "⟁COMPUTE_FOLD⟁"
bound_lane      = "BATCH"
cm1_gate        = "U+0002"

# Inference constraints
max_tokens          = 512
temperature         = 0.0
top_p               = 1.0
top_k               = 1
repetition_penalty  = 1.0
decode_strategy     = "greedy_argmax"

# MoE architecture
[model.s7_llm_moe_140m.trunk]
hidden_dim  = 768
num_layers  = 6
num_heads   = 12
head_dim    = 64
ffn_dim     = 3072
rope        = true
param_count = "~20M"

[model.s7_llm_moe_140m.experts]
count            = 4
hidden_dim       = 512
num_layers       = 8
num_heads        = 8
head_dim         = 64
ffn_dim          = 2048
param_count_each = "~28M"
domains          = ["CODE", "MATH", "REASON", "GENERAL"]

[model.s7_llm_moe_140m.router]
type            = "deterministic_lexical"
learned         = false
code_triggers   = ["{", "}", "def ", "fn ", "class ", "::", ";", "import "]
math_triggers   = ["Answer:", "Therefore", "=", "solve", "equation"]
reason_triggers = ["why ", "explain ", "because ", "step ", "conclude"]
fallback        = "GENERAL"

# Memory footprint
[model.s7_llm_moe_140m.memory]
fp32_training_bytes          = 560000000
int8_inference_bytes         = 150000000
kv_cache_full_context_bytes  = 35700000
webgpu_total_bytes           = 200000000

# SCXQ2 .s7l lane layout
[model.s7_llm_moe_140m.s7l_lanes]
lane_1_DICT  = "BPE vocabulary (24576 entries)"
lane_2_FIELD = "INT8 weight tensors (trunk + 4 experts, AVX2-padded)"
lane_3_LANE  = "Generation stream placeholder"
lane_4_EDGE  = "Sub-Merkle roots (trunk + expert0-3) + routing table"
lane_5_BATCH = "Ephemeral compute placeholder"

# Merkle structure
[model.s7_llm_moe_140m.merkle]
algorithm      = "SHA-256"
root_covers    = ["trunk", "expert0", "expert1", "expert2", "expert3"]
partial_verify = true
header_offset  = 8

# Runtime targets
[model.s7_llm_moe_140m.runtime_targets]
cpu_avx2 = true
webgpu   = true
wasm     = false
cuda     = false

# Training provenance (to be filled after training)
[model.s7_llm_moe_140m.provenance]
training_spec             = "docs/s7-llm-moe-training.md"
phase1_tokens             = "50B"
phase2_tokens_per_expert  = "~15B"
total_gpu_hours           = "~14000"
quantization_method       = "per_row_int8_ptq"
sealed                    = false

# ============================================================================
# S7-LLM-MOE-300M: True Learned MoE — 9 Neural Experts (one per micronaut)
# ============================================================================

[model.s7_llm_moe_300m]
name = "s7-llm-moe-300m"
class = "SCXQ7::S7_LLM"
variant = "LEARNED_MOE"
format = "s7l"
version = "2.0.0"
parameters_total = "~300M"
parameters_active_per_token = "~104M"
context_length = 2048
vocab_size = 32768
architecture = "learned_mixture_of_experts"
quantization = "INT8"
training_dtype = "BF16"
deterministic = true
fold_enclosed = true
ownership = "static"

# File paths
artifact_path = "s7-llm-moe/model/moe-300m.s7l"
vocab_path    = "s7-llm-moe/model/vocab.json"
runtime_bin   = "s7-llm-moe/target/release/s7-llm-moe"
packer_bin    = "s7-llm-moe/target/release/s7-pack-moe"
training_dir  = "training/"
training_spec = "docs/s7-llm-moe-training.md"

# Fold binding
bound_micronaut = "MM-1"
bound_fold      = "⟁COMPUTE_FOLD⟁"
bound_lane      = "BATCH"
cm1_gate        = "U+0002"

# Inference constraints (deterministic)
max_tokens         = 512
temperature        = 0.0
decode_strategy    = "greedy_argmax"
router_strategy    = "argmax"           # NOT softmax sampling

# Training hyperparameters
[model.s7_llm_moe_300m.training]
optimizer      = "AdamW"
lr             = 0.0001
weight_decay   = 0.1
batch_size     = 512
seq_len        = 2048
grad_accum     = 4
phase1_steps   = 200000
phase2_steps   = 300000
total_steps    = 500000
alpha_balance  = 0.01    # Load-balancing loss weight
beta_entropy   = 0.001   # Router entropy regularizer weight
precision      = "bfloat16"
phase1_routing = "uniform"   # Dense pretrain: all experts contribute equally
phase2_routing = "top1"      # Sparse MoE: argmax routing, specialization emerges

# Architecture: SharedTrunk
[model.s7_llm_moe_300m.trunk]
hidden_dim  = 1024
num_layers  = 12
num_heads   = 16
head_dim    = 64
ffn_dim     = 4096
rope        = true
param_count = "~80M"

# Architecture: LearnedRouter
[model.s7_llm_moe_300m.router]
type          = "learned_mlp"
learned       = true
hidden_dim    = 2048
input_dim     = 1024
output_dim    = 9
activation    = "gelu"
inference     = "argmax"    # deterministic at inference
param_count   = "~2M"

# Architecture: 9 Experts (one per micronaut/fold)
[model.s7_llm_moe_300m.experts]
count            = 9
hidden_dim       = 1024
num_layers       = 4
num_heads        = 16
head_dim         = 64
ffn_dim          = 4096
param_count_each = "~24M"
specialization   = "emergent"   # NOT pre-assigned; emerges from routing
routing_mode     = "top1_argmax"

[[model.s7_llm_moe_300m.experts.micronaut_map]]
expert_id  = 0
micronaut  = "PM-1"
fold       = "⟁DATA_FOLD⟁"
role       = "field_selection"

[[model.s7_llm_moe_300m.experts.micronaut_map]]
expert_id  = 1
micronaut  = "CM-1"
fold       = "⟁CONTROL_FOLD⟁"
role       = "phase_geometry"

[[model.s7_llm_moe_300m.experts.micronaut_map]]
expert_id  = 2
micronaut  = "TM-1"
fold       = "⟁TIME_FOLD⟁"
role       = "collapse_timing"

[[model.s7_llm_moe_300m.experts.micronaut_map]]
expert_id  = 3
micronaut  = "HM-1"
fold       = "⟁STATE_FOLD⟁"
role       = "host_abstraction"

[[model.s7_llm_moe_300m.experts.micronaut_map]]
expert_id  = 4
micronaut  = "MM-1"
fold       = "⟁COMPUTE_FOLD⟁"
role       = "token_signal_generator"

[[model.s7_llm_moe_300m.experts.micronaut_map]]
expert_id  = 5
micronaut  = "XM-1"
fold       = "⟁PATTERN_FOLD⟁"
role       = "narrative_expansion"

[[model.s7_llm_moe_300m.experts.micronaut_map]]
expert_id  = 6
micronaut  = "SM-1"
fold       = "⟁STORAGE_FOLD⟁"
role       = "inert_persistence"

[[model.s7_llm_moe_300m.experts.micronaut_map]]
expert_id  = 7
micronaut  = "VM-2"
fold       = "⟁META_FOLD⟁"
role       = "proof_generation"

[[model.s7_llm_moe_300m.experts.micronaut_map]]
expert_id  = 8
micronaut  = "VM-1"
fold       = "⟁UI_FOLD⟁"
role       = "rendering_projection"

# Memory footprint
[model.s7_llm_moe_300m.memory]
fp32_training_bytes         = 1200000000   # 1.2GB (model + gradients + optimizer)
int8_inference_bytes        = 300000000    # ~300MB
kv_cache_trunk_bytes        = 50300000     # 2048 × 12L × 2 × 1024
kv_cache_expert_bytes       = 16800000     # 2048 × 4L × 2 × 1024 (one expert)
webgpu_total_bytes          = 380000000    # ~380MB total

# SCXQ2 .s7l lane layout
[model.s7_llm_moe_300m.s7l_lanes]
lane_1_DICT  = "BPE vocabulary (32768 entries)"
lane_2_FIELD = "INT8 weight tensors (trunk + router + 9 experts, AVX2-padded)"
lane_3_LANE  = "Generation stream placeholder"
lane_4_EDGE  = "Sub-Merkle roots (trunk + router + expert0-8)"
lane_5_BATCH = "Ephemeral compute placeholder"

# Merkle / proof structure
[model.s7_llm_moe_300m.proof]
algorithm      = "SHA-256"
root_covers    = ["trunk", "router", "expert0", "expert1", "expert2",
                  "expert3", "expert4", "expert5", "expert6", "expert7", "expert8"]
partial_verify = true
proof_chain    = true    # per-token step_hash chained into cumulative chain_hash
router_hash    = true    # SHA-256(router_logits_i8) included in each step

# Performance estimates
[model.s7_llm_moe_300m.benchmarks]
HumanEval  = "18-28%"
GSM8K      = "15-25%"
MMLU       = "35-45%"
ARC_Challenge = "30-40%"

# Provenance
[model.s7_llm_moe_300m.provenance]
training_spec        = "docs/s7-llm-moe-training.md"
phase1_tokens        = "50B"
phase2_tokens        = "60B"
total_gpu_hours      = "~16000"
quantization_method  = "per_row_absmax_int8"
sealed               = false

# ============================================================================
# S7-LLM-MOE-5B: Competitive-Scale MoE — 9 Neural Experts, 1.5B Shared Trunk
# ============================================================================
# Total parameters ≈ 5.1B
#   Shared Trunk   ≈ 1.5B  (32L, 2560d, 40 heads)
#   9 × Expert     ≈ 380M  (16L, 2560d, 40 heads, 4× FFN)
#   Router         ≈  60M  (2560→4096 GELU→9)
# Active per token (Top-1 inference): ~1.9B (trunk + 1 expert)
# Training routing: Top-2 (gradient stability)
# Inference routing: Top-1 argmax (deterministic)
# ============================================================================

[model.s7_llm_moe_5b]
name = "s7-llm-moe-5b"
class = "SCXQ7::S7_LLM"
variant = "COMPETITIVE_MOE"
format = "s7l"
version = "3.0.0"
parameters_total = "~5.1B"
parameters_trunk = "~1.5B"
parameters_per_expert = "~380M"
parameters_router = "~60M"
parameters_active_per_token = "~1.9B"
expert_count = 9
context_length = 8192
vocab_size = 50000
architecture = "large_scale_mixture_of_experts"
quantization = "INT8"
training_dtype = "BF16"
deterministic = true
fold_enclosed = true
ownership = "static"

# File paths (relative to project root)
artifact_path = "s7-llm-moe/model/moe-5b.s7l"
vocab_path    = "s7-llm-moe/model/vocab-50k.json"
runtime_bin   = "s7-llm-moe/target/release/s7-llm-moe"
packer_bin    = "s7-llm-moe/target/release/s7-pack-moe"
training_dir  = "training/"
training_spec = "docs/s7-llm-moe-5b-training.md"

# Fold binding
bound_micronaut = "MM-1"
bound_fold      = "⟁COMPUTE_FOLD⟁"
bound_lane      = "BATCH"
cm1_gate        = "U+0002"

# Inference constraints (deterministic)
max_tokens         = 2048
temperature        = 0.0
decode_strategy    = "greedy_argmax"
router_strategy    = "argmax"           # Top-1 argmax at inference (NOT softmax sampling)
training_router_strategy = "top2"       # Top-2 during training for gradient stability

# Training hyperparameters
[model.s7_llm_moe_5b.training]
optimizer         = "AdamW"
lr                = 0.0001
lr_schedule       = "cosine_with_warmup"
warmup_steps      = 2000
weight_decay      = 0.1
batch_size        = 512
seq_len           = 8192
grad_accum        = 8
phase1_steps      = 300000
phase2_steps      = 400000
total_steps       = 700000
alpha_balance     = 0.01           # Load-balancing loss weight (L_balance = 9 × Σ(f_i · g_i))
beta_entropy_init = 0.01           # Initial entropy annealing coefficient
beta_entropy_decay = "exponential" # β_t = β_initial × exp(-k × t)
beta_entropy_k    = 0.000005       # Decay rate k
precision         = "bfloat16"
phase1_routing    = "top2"         # Dense pretrain: top-2 experts, combined weighted output
phase2_routing    = "top1"         # Sparse specialization: argmax routing emerges
min_gpu_count     = 16             # Minimum 16× A100 80GB for training
recommended_gpu_count = 32
parallelism       = ["ZeRO-3", "expert_parallel", "tensor_parallel"]
training_tokens_phase1 = "100B"
training_tokens_phase2 = "100B"
total_training_tokens  = "200B"

# Architecture: SharedTrunk (~1.5B)
# Comparable to GPT-2 XL class — handles syntax, long-range coherence, cross-domain abstraction
[model.s7_llm_moe_5b.trunk]
hidden_dim  = 2560
num_layers  = 32
num_heads   = 40
head_dim    = 64
ffn_dim     = 10240
rope        = true
rope_base   = 10000
param_count = "~1.5B"

# Architecture: LearnedRouter (~60M)
# Large learned router — powerful enough to distinguish 9 expert domains at 5B scale
[model.s7_llm_moe_5b.router]
type          = "learned_mlp"
learned       = true
input_dim     = 2560
hidden_dim    = 4096
output_dim    = 9
activation    = "gelu"
inference     = "argmax"           # Top-1 deterministic at inference
training      = "top2_softmax"     # Top-2 weighted combine during training
param_count   = "~60M"
optional_token_type_embedding = true

# Architecture: 9 Experts (~380M each, ~3.42B total)
# At 5B scale, experts diverge dramatically: code, math, reasoning, formatting,
# factual recall, hallucination suppression, narrative, instruction, meta-reasoning.
# Specialization is emergent from routing — NOT pre-assigned.
[model.s7_llm_moe_5b.experts]
count            = 9
hidden_dim       = 2560
num_layers       = 16
num_heads        = 40
head_dim         = 64
ffn_multiplier   = 4
ffn_dim          = 10240
param_count_each = "~380M"
param_count_total = "~3.42B"
specialization   = "emergent"      # Emerges from training; NOT hard-coded
routing_mode     = "top1_argmax"   # Deterministic at inference

[[model.s7_llm_moe_5b.experts.micronaut_map]]
expert_id  = 0
micronaut  = "PM-1"
fold       = "⟁DATA_FOLD⟁"
role       = "field_selection"

[[model.s7_llm_moe_5b.experts.micronaut_map]]
expert_id  = 1
micronaut  = "CM-1"
fold       = "⟁CONTROL_FOLD⟁"
role       = "phase_geometry"

[[model.s7_llm_moe_5b.experts.micronaut_map]]
expert_id  = 2
micronaut  = "TM-1"
fold       = "⟁TIME_FOLD⟁"
role       = "collapse_timing"

[[model.s7_llm_moe_5b.experts.micronaut_map]]
expert_id  = 3
micronaut  = "HM-1"
fold       = "⟁STATE_FOLD⟁"
role       = "host_abstraction"

[[model.s7_llm_moe_5b.experts.micronaut_map]]
expert_id  = 4
micronaut  = "MM-1"
fold       = "⟁COMPUTE_FOLD⟁"
role       = "token_signal_generator"

[[model.s7_llm_moe_5b.experts.micronaut_map]]
expert_id  = 5
micronaut  = "XM-1"
fold       = "⟁PATTERN_FOLD⟁"
role       = "narrative_expansion"

[[model.s7_llm_moe_5b.experts.micronaut_map]]
expert_id  = 6
micronaut  = "SM-1"
fold       = "⟁STORAGE_FOLD⟁"
role       = "inert_persistence"

[[model.s7_llm_moe_5b.experts.micronaut_map]]
expert_id  = 7
micronaut  = "VM-2"
fold       = "⟁META_FOLD⟁"
role       = "proof_generation"

[[model.s7_llm_moe_5b.experts.micronaut_map]]
expert_id  = 8
micronaut  = "VM-1"
fold       = "⟁UI_FOLD⟁"
role       = "rendering_projection"

# Load balancing objective
# L_balance = 9 × Σ(f_i × g_i)  where f_i = fraction of tokens routed to expert i,
# g_i = average routing probability for expert i
# Entropy annealing: β_t = β_initial × exp(-k × t)
# Early training → high entropy (exploration); later → low entropy (specialization)
[model.s7_llm_moe_5b.load_balancing]
formula             = "n_experts × sum(f_i × g_i)"
n_experts           = 9
loss_coefficient    = 0.01
entropy_annealing   = true
entropy_schedule    = "exponential_decay"
entropy_formula     = "beta_initial × exp(-k × t)"
early_phase         = "exploration"     # High entropy: tokens distributed across all experts
late_phase          = "specialization"  # Low entropy: clear expert ownership emerges

# Memory footprint
[model.s7_llm_moe_5b.memory]
fp32_weights_bytes             = 20400000000   # 5.1B × 4 bytes ≈ 20.4GB
fp32_adam_states_bytes         = 61200000000   # ≈ 3× weights ≈ 61.2GB
fp32_gradients_bytes           = 20400000000   # ≈ 1× weights ≈ 20.4GB
fp32_training_total_bytes      = 160000000000  # ~160GB with activations
int8_inference_bytes           = 5100000000    # 5.1B × 1 byte ≈ 5.1GB
int8_scale_buffers_bytes       = 500000000     # ~500MB scale factors
int8_inference_total_bytes     = 6000000000    # ~6GB comfortable on 24GB GPU
kv_cache_trunk_int8_bytes      = 1342177280    # 8192 × 32L × 2 × 2560
kv_cache_expert_int8_bytes     = 671088640     # 8192 × 16L × 2 × 2560 (one expert)
recommended_inference_vram     = "24GB"
min_training_gpu_vram          = "80GB"

# SCXQ2 .s7l lane layout
[model.s7_llm_moe_5b.s7l_lanes]
lane_1_DICT  = "BPE vocabulary (50000 entries)"
lane_2_FIELD = "INT8 weight tensors (trunk + router + 9 experts, AVX2-padded)"
lane_3_LANE  = "Generation stream placeholder"
lane_4_EDGE  = "Sub-Merkle roots (trunk + router + expert0-8) + routing table"
lane_5_BATCH = "Ephemeral compute placeholder"

# Merkle / proof structure
[model.s7_llm_moe_5b.proof]
algorithm      = "SHA-256"
root_covers    = ["trunk", "router", "expert0", "expert1", "expert2",
                  "expert3", "expert4", "expert5", "expert6", "expert7", "expert8"]
partial_verify = true
proof_chain    = true    # per-token step_hash chained into cumulative chain_hash
router_hash    = true    # SHA-256(router_logits_i8) included in each step
expert_hash    = true    # SHA-256(selected_expert_weights) attested per inference

# Governance / fold-law integration
# At this scale the governance layer wraps a real large neural cortex:
# expert activation log, router logits hash, expert weight hash,
# deterministic inference envelope, replayable proof chain.
[model.s7_llm_moe_5b.governance]
expert_activation_log  = true
router_logits_hash     = true
expert_weight_hash     = true
deterministic_envelope = true
replayable_proof_chain = true
vm2_attestation_required = true
cm1_gate_required        = true

# Performance estimates (rivaling LLaMA-1 7B / early Mistral-class on 200B+ tokens)
[model.s7_llm_moe_5b.benchmarks]
HumanEval      = "50-65%"
GSM8K          = "50-65%"
MMLU           = "60-70%"
ARC_Challenge  = "55-65%"
long_context   = "strong"
multi_hop      = "moderate_to_strong"
notes          = "Estimates assume 200B+ training tokens; rivals LLaMA-1 7B and early Mistral-class"

# Runtime targets
[model.s7_llm_moe_5b.runtime_targets]
cpu_avx2  = false    # Too large for practical CPU inference
webgpu    = false    # Exceeds WebGPU memory limits
wasm      = false
cuda_a100 = true
cuda_h100 = true
multi_gpu = true

# Provenance
[model.s7_llm_moe_5b.provenance]
training_spec        = "docs/s7-llm-moe-5b-training.md"
phase1_tokens        = "100B"
phase2_tokens        = "100B"
total_gpu_hours      = "~80000"
quantization_method  = "per_row_absmax_int8"
sealed               = false
